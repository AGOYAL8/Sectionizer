{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [13/May/2020 15:26:36] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "127.0.0.1 - - [13/May/2020 15:26:59] \"\u001b[37mPOST /select_model HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/May/2020 15:27:02] \"\u001b[37mPOST /output HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/May/2020 15:27:10] \"\u001b[37mPOST /section_view HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/May/2020 15:27:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [13/May/2020 15:27:12] \"\u001b[37mPOST /download_section HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/May/2020 15:27:19] \"\u001b[37mPOST /download_output HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Python and WSGI libraries\n",
    "from flask import Flask, render_template, request, send_file\n",
    "from werkzeug.utils import secure_filename\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")      # to neglect warning messages\n",
    "\n",
    "app = Flask(__name__)       \n",
    "\n",
    "@app.route('/')\n",
    "def upload():\n",
    "    return render_template(\"upload-docx.html\")        # default html to get data from the user into a html form.\n",
    "\n",
    "@app.route('/select_model', methods = ['GET', 'POST']) # gets form POST data from html form into the backend for processing.\n",
    "def nlp_algos():                                      # function enables the data preprocessing and generates vector space on the input file.\n",
    "    if request.method == 'POST':\n",
    "        f = request.files['document']                 # the input file is captured and saved on the system using WERKZEUG toolkit.\n",
    "        f.save(secure_filename(f.filename))\n",
    "        loc = '/Users/anshulgoyal/Downloads/Nowigence/' + f.filename   # location of the input file\n",
    "        loc = loc.replace(\" \",\"_\")                                     # adding the changes made in the filename on the system due to the toolkit\n",
    "        \n",
    "        # python libraries\n",
    "        from docx import Document\n",
    "        from joblib import load, dump\n",
    "        import pandas as pd\n",
    "        from nltk import word_tokenize\n",
    "        from nltk.stem.porter import PorterStemmer\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        import re\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from sklearn.feature_extraction.text import HashingVectorizer\n",
    "        from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "        import gensim\n",
    "        from gensim import models\n",
    "        from gensim.matutils import softcossim\n",
    "        from gensim import corpora\n",
    "        import gensim.downloader as api\n",
    "        from gensim.utils import simple_preprocess\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        stemmer = PorterStemmer()                                         # Using NLTK library\n",
    "        lemmatizer = WordNetLemmatizer()                                  # Using NLTK library\n",
    "\n",
    "        # model = api.load('glove-wiki-gigaword-300')                       # Using GENSIM library, GLOVE embeddings of wikipedia dataset with 300 dimensions. ###### Time complexity to load ~ 3 mins (observed)\n",
    "        # fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')   # Using GENSIM library, fastText embeddings of wikipedia news with 300 dimensions. ###### Time complexity to load ~ 7 mins (observed)\n",
    "        \n",
    "        def read_files(path):                                             # here we take the input docx file with desired constraints to perform NLP tasks to get vector spaces as testing data.\n",
    "            document = Document(path)                                     # docx file is read from the system location which is passed when this function is called.\n",
    "            doc = []                                                      # local scope list to capture paragraphs\n",
    "            for para in document.paragraphs:                              # code to remove paragraphs shorter than 201 characters\n",
    "                if len(para.text) > 201:\n",
    "                    doc.append(para.text)\n",
    "            df = pd.DataFrame(doc, columns=['sent'])                      # \"df\" dataframe to perform next task\n",
    "            return df\n",
    "        \n",
    "        def tokenize(x, stem=False, lemma=False):                         # here the tokenised data is filtered from any unnecessary characters.\n",
    "            x = re.sub(r'(?:<[^>]+>)', '', x)                             # Using Regular Expression\n",
    "            pattern = '0-9 $ ` % {} <> -,: _ \\ . = +| /'\n",
    "            x = re.sub(r'(pattern)', '', x)\n",
    "            tokens = word_tokenize(x)                                     # tokenizing step, word tokenizer called from the NLTK library \n",
    "            if stem:                                                      # stemming step, Porter stemmer called from the NLTK library\n",
    "                tokens = [stemmer.stem(t) for t in tokens]\n",
    "            if lemma:                                                     # lemmatizing step, Wordnet lemmatizer called from the NLTK library\n",
    "                tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            return tokens                                                 # returns tokens to the preprocess function\n",
    "        \n",
    "        def preprocess(df, stem=False, lemma=False):                      # here we pass which root word algorithm to perform in the data preprocessing task\n",
    "            tokens = []\n",
    "            if stem:\n",
    "                tokens = df.sent.apply(lambda x: tokenize(x, True))        # Calls the TOKENIZE function\n",
    "            if lemma:\n",
    "                tokens = df.sent.apply(lambda x: tokenize(x, False, True)) # Calls the TOKENIZE function\n",
    "            return tokens                                                 # this returns the preprocessed tokens for next step\n",
    "        \n",
    "        def cosine(tokens, stem=False, lemma=False):                      # here we apply cosine similarity using 2 Vectorizer's as mentioned below\n",
    "            cosout_tfidf = []\n",
    "            cosout_cv = []\n",
    "            tokens = tokens.apply(lambda x: ' '.join(x))                  # tokens are joined to text again\n",
    "            for count in range(0, len(tokens) - 1):                       # for loop to create Bag of Words and apply Cosine Similarity \n",
    "                sent1 = tokens[count]\n",
    "                sent2 = tokens[count + 1]\n",
    "                documents = [sent1, sent2]                                # list of items to be compared to get vector spaces.\n",
    "                tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))   # TfIdf-Vectorizer from SKLEARN library over Bag of Words ranging 1 to 3 (unigram till trigram)\n",
    "                tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "                tfidf = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)                     # Cosine Similarity from SKLEARN library over TF-IDF values\n",
    "                cosout_tfidf.append(tfidf[0][1])\n",
    "                count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 3))   # Count-Vectorizer from SKLEARN library over Bag of Words ranging 1 to 3 (unigram till trigram)\n",
    "                sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "                cv = cosine_similarity(sparse_matrix[0:1], sparse_matrix)                      # Cosine Similarity from SKLEARN library over Count Vectorize values\n",
    "                cosout_cv.append(cv[0][1])\n",
    "            global df_cosine                                                                   # dataframe made gloablly available for future use and debugging\n",
    "            if stem:\n",
    "                df_cosine = pd.DataFrame(list(zip(cosout_tfidf, cosout_cv)),\n",
    "                                  columns=['stem_cosout_tfidf', 'stem_cosout_cv'])\n",
    "            if lemma:\n",
    "                df_cosine = pd.DataFrame(list(zip(cosout_tfidf, cosout_cv)),\n",
    "                                  columns=['lemma_cosout_tfidf', 'lemma_cosout_cv'])\n",
    "            return(df_cosine)\n",
    "        \n",
    "        def glove_sofcos(tokens, stem=False, lemma=False):               # here we apply Soft cosine similarity using GLOVE embeddings\n",
    "            glsoftcosout = []\n",
    "            tokens = tokens.apply(lambda x: ' '.join(x))\n",
    "            for count in range(0, len(tokens)-1):\n",
    "                sent1 = tokens[count]\n",
    "                sent2 = tokens[count+1]\n",
    "                documents = [sent1, sent2]\n",
    "                dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents]) # GENSIM library is used to get corpora, simple_preprocess functions\n",
    "                similarity_matrix = model.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0,\n",
    "                                                                        nonzero_limit=100)\n",
    "                sent_1 = dictionary.doc2bow(simple_preprocess(sent1))\n",
    "                sent_2 = dictionary.doc2bow(simple_preprocess(sent2))\n",
    "                soft_cosine_output = softcossim(sent_1, sent_2, similarity_matrix)             # Using GENSIM library, softcossim is called\n",
    "                glsoftcosout.append(soft_cosine_output)\n",
    "            global df_glsoftcos\n",
    "            if stem:\n",
    "                df_glsoftcos = pd.DataFrame(glsoftcosout, columns = ['stem_Glove_Soft_Cos'])\n",
    "            if lemma:\n",
    "                df_glsoftcos = pd.DataFrame(glsoftcosout, columns = ['lemma_Glove_Soft_Cos'])\n",
    "            return(df_glsoftcos)\n",
    "        \n",
    "        def fasttext_sofcos(tokens, stem=False, lemma=False):           # here we apply Soft cosine similarity using fastTEXT embeddings\n",
    "            softcosout = []\n",
    "            tokens = tokens.apply(lambda x: ' '.join(x))\n",
    "            for count in range(0, len(tokens)-1):\n",
    "                sent1 = tokens[count]\n",
    "                sent2 = tokens[count+1]\n",
    "                documents = [sent1, sent2]\n",
    "                dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "                similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0,\n",
    "                                                                        nonzero_limit=100)\n",
    "                sent_1 = dictionary.doc2bow(simple_preprocess(sent1))\n",
    "                sent_2 = dictionary.doc2bow(simple_preprocess(sent2))\n",
    "                soft_cosine_output = softcossim(sent_1, sent_2, similarity_matrix)            # Using GENSIM library, softcossim is called    \n",
    "                softcosout.append(soft_cosine_output)\n",
    "            global df_softcos    \n",
    "            if stem:\n",
    "                df_softcos = pd.DataFrame(softcosout, columns = ['stem_fastText_Soft_cosine'])\n",
    "            if lemma:\n",
    "                df_softcos = pd.DataFrame(softcosout, columns = ['lemma_fastText_Soft_cosine'])\n",
    "            return(df_softcos)\n",
    "        \n",
    "        def run(stem=False, lemma=False):\n",
    "            global df_doc, x, y, z      # global scope for future use in next app.route and debugging purpose\n",
    "            df_doc = read_files(loc)    # Calls the READ_FILES function passing the file path as 'loc' ----- value in 'loc' was captured just after the input file was saved using secure_filename.\n",
    "            tokens = preprocess(df_doc, stem=stem, lemma=lemma)     # Calls the PREPROCESS function, while passing df_doc to the function\n",
    "            x = cosine(tokens, stem=stem, lemma=lemma)              # Calls the COSINE function, while passing tokens to the function\n",
    "            # y = fasttext_sofcos(tokens, stem=stem, lemma=lemma)     # Calls the FASTTEXT_SOFCOS function, while passing tokens to the function                           ## time_complexity ~ 10 mins for data size of 20 rows (observed)\n",
    "            # z = glove_sofcos(tokens, stem=stem, lemma=lemma)        # Calls the GLOVE_SOFCOS function, while passing tokens to the function                           ## time_complexity ~ 05 mins for data size of 20 rows (observed)\n",
    "        \n",
    "        def run_all():\n",
    "            global result               # global scope for future use in next app.route and debugging purpose\n",
    "            result = pd.DataFrame()\n",
    "            run(stem=True, lemma=False)                       # Calls the RUN function selecting stemming\n",
    "            result = pd.concat([x], axis=1, sort=False)   # Since only using x - \"result = pd.concat([x, y, z], axis=1, sort=False)\"\n",
    "            run(stem=False, lemma=True)                       # Calls the RUN function selecting lemmatizing\n",
    "            result = pd.concat([result, x], axis=1, sort=False)    # change when using y and z also...    \n",
    "        run_all()                ### executing the RUN_ALL function, once the input file is saved.\n",
    "        return render_template(\"select-model.html\")           # redirects the user to select-model.html for getting which ML model to be performed. \n",
    "\n",
    "    \n",
    "@app.route('/output', methods = ['GET', 'POST'])       # gets form POST data from html form into the backend for ML modeling.\n",
    "def ml_models():\n",
    "    if request.method == 'POST':\n",
    "        model = request.form.get('Model')              # we get the choice made by the user as 'model'\n",
    "        import joblib\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import time\n",
    "        \n",
    "####### NOTE:- PICKLED MODEL NEEDS TO BE UPDATED FROM THE ML_MODEL_PYTHON CODE FILE IF GLOVE AND FASTTEXT EMBEDDINGS ARE INCLUDED. \n",
    "        \n",
    "        if model == 'model':                           # when user doesnot selects any model, the html throws exception and flask redirects the user to the make a choice again.\n",
    "            return render_template(\"select-model.html\")   \n",
    "        if model == 'lasso':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/lasso_model.pkl\")      # calling lasso model using pickled file placed in the same directory where the flask code exists. Likewise other models\n",
    "        if model == 'ridge':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/ridge_model.pkl\")            \n",
    "        if model == 'tree':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/decision_tree_model.pkl\")            \n",
    "        if model == 'perceptron':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/perceptron_model.pkl\")            \n",
    "        if model == 'logit':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/logistic_regression_model.pkl\")            \n",
    "        if model == 'nBayes':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/naive_bayes_model.pkl\")            \n",
    "        if model == 'forest':\n",
    "            model_name = joblib.load(\"/Users/anshulgoyal/Downloads/Nowigence/random_forest_model.pkl\")\n",
    "        ### global scope variables created for other app route access to the information saved in these variables\n",
    "        global predictions, df, output, timestr   \n",
    "        predictions = model_name.predict(result)  # Calls predict function existing in the pickled model.\n",
    "        if model == 'lasso':                                    ## specific to the lasso model\n",
    "            predictions = np.where(predictions > 0.5, 1, 0)     ## To convert Probabilities.\n",
    "        master_list = []                                        ## List created to print the paragraphs into the dataframe\n",
    "        for index, rows in df_doc.iterrows():                   ## df_doc is read to get the original text in the input file given by the user.\n",
    "            mylist1 = rows.sent\n",
    "            master_list.append(mylist1)\n",
    "        ### gloabl scope dataframe used for creating another view later to display sections.\n",
    "        df = pd.DataFrame({'S/N': range(1,len(predictions)+1),'Paragraph':master_list[0:len(predictions)], 'Compared to':master_list[1:len(predictions)+1], 'Section': predictions}) \n",
    "        timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")            ## time stamp assigned to uniquely identify the output generated by the models.\n",
    "        df.to_csv('output.'+ timestr, index=False)              ## unique name assigned for future download services\n",
    "        return render_template('third.html',  tables = [df.to_html(classes='data', header=True, index=False)])\n",
    "    \n",
    "@app.route('/download_output', methods = ['GET', 'POST'])   ## DOWNLOAD SERVICE CODE\n",
    "def download_output():                                      ## once user selects to download CSV file this function traces the file and returns it to the end-user using send_file function\n",
    "    if request.method == 'POST':\n",
    "        path = '/Users/anshulgoyal/Downloads/Nowigence/' + 'output.'+ timestr\n",
    "        return send_file(path, mimetype='text/csv', as_attachment=True)\n",
    "\n",
    "@app.route('/section_view', methods = ['GET', 'POST'])    ## to avail another view of the dataframe \"df\" generated using the predictions.\n",
    "def para_view():                                          ## When user selects to view SETIONIZED VIEW on the webpage\n",
    "    if request.method == 'POST': \n",
    "        import pandas as pd\n",
    "        l = []                                            # local scope list 'l' to append section boundary in the dataframe\n",
    "        for i in range(len(df)):\n",
    "            if df.loc[i,'Section'] == 0:                  # if prediction column is ZERO\n",
    "                l.append(df.loc[i,'Paragraph'])\n",
    "            else:                                         # if prediction column is ONE\n",
    "                l.append(df.loc[i,'Paragraph'])\n",
    "                l.append(\"-    - - - !!! - - -    -\")     #######   SECTION DIVIDER STRING   #######\n",
    "        df2 = pd.DataFrame({'SECTION': l})\n",
    "        df2.to_csv('section.'+ timestr, index=False)\n",
    "        return render_template('final.html',  tables = [df2.to_html(classes='data', header=True, index=False)])\n",
    "\n",
    "@app.route('/download_section', methods = ['GET', 'POST'])  ## DOWNLOAD SERVICE CODE\n",
    "def download_section():                                     ## once user selects to download CSV file this function traces the file and returns it to the end-user using send_file function\n",
    "    if request.method == 'POST':    \n",
    "        path = '/Users/anshulgoyal/Downloads/Nowigence/' + 'section.'+ timestr\n",
    "        return send_file(path, mimetype='text/csv', as_attachment=True)         \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
